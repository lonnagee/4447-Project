{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "In the era of digital transformation, online reviews have become a powerful driving force influencing consumer choices and business reputation. Yelp, a prominent user-generated review platform, houses a vast repository of valuable insights encapsulated within millions of reviews. This project delves into the Yelp review dataset, employing data cleaning and natural language processing techniques to uncover hidden patterns and sentiments. By analyzing this rich textual data, we aim to assist businesses in understanding customer feedback, enhancing their services, and gaining an edge in an ever-competitive market. Let us embark on a journey of data exploration and sentiment analysis, revealing the untold story behind Yelp's myriad reviews.\n",
    "\n",
    "## Motivation and Dataset Description:\n",
    "Yelp is a popular online platform where users can provide reviews and ratings for businesses ranging from restaurants, cafes, and bars to local services and shops. The Yelp review dataset offers a wealth of information from millions of user-generated reviews, making it a valuable resource for conducting data-driven analyses and gaining insights into customer sentiments and behavior.\n",
    "\n",
    "The primary motivation behind this project is to explore the Yelp review dataset and leverage natural language processing (NLP) techniques to uncover patterns, sentiments, and trends hidden within the vast volume of textual data. By harnessing NLP, we aim to extract valuable information from the reviews, such as sentiment polarity, key phrases, and common topics, that can aid businesses in understanding customer feedback and enhancing their services.\n",
    "\n",
    "## Dataset Description:\n",
    "The Yelp review dataset is a comprehensive collection of user reviews, encompassing multiple attributes such as user IDs, business IDs, review text, and star ratings. The dataset spans diverse geographical locations, businesses, and user demographics, providing a rich and varied set of reviews.\n",
    "\n",
    "The main columns in the dataset include:\n",
    "\n",
    "- `review_id`: A unique identifier for each review.\n",
    "- `user_id`: The identifier of the user who wrote the review.\n",
    "- `business_id`: The identifier of the business being reviewed.\n",
    "- `stars`: The star rating given by the user (ranging from 1 to 5).\n",
    "- `useful`, `funny`, `cool`: The counts of how many users marked the review as useful, funny, or cool.\n",
    "- `text`: The textual content of the review.\n",
    "- `date`: The date and time when the review was posted.\n",
    "\n",
    "The dataset contains millions of reviews, making it a massive corpus of text that can be mined for valuable insights. However, like any real-world dataset, it also presents challenges, such as missing data, potential outliers, and the need for proper data cleaning and preprocessing.\n",
    "\n",
    "By analyzing the Yelp review dataset, we aim to unravel patterns of user behavior, perform sentiment analysis to gauge customer sentiments, and contribute to a better understanding of user feedback on various businesses.\n",
    "\n",
    "## Research Question\n",
    "How well can sentiment analysis accurately predict the sentiment polarity of Yelp reviews compared to the user-provided star ratings?\n",
    "\n",
    "In this study, we aim to employ Natural Language Processing (NLP) techniques, such as the VADER SentimentIntensityAnalyzer, to extract sentiment scores from the textual content of Yelp reviews. The research question focuses on comparing these sentiment scores with the star ratings assigned by users to assess the level of alignment between sentiment and rating.\n",
    "\n",
    "Through this analysis, we seek to explore whether the extracted sentiments closely align with the star ratings or if there are instances of discrepancies. By validating the sentiment analysis against user ratings, we aim to gain insights into the efficacy of NLP methods for capturing sentiment from reviews and uncover any underlying patterns in the user feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m'\u001b[39m\u001b[39m../src\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mextract_data\u001b[39;00m \u001b[39mimport\u001b[39;00m process_json_files\n",
      "File \u001b[0;32m/mnt/c/Users/corey/Desktop/Education/University of Denver/Classes/COMP4447_Data Science Tools 1/Final Project/4447-Project/src/extract_data.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m                 df\u001b[39m.\u001b[39mto_csv(csv_file_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSaved \u001b[39m\u001b[39m{\u001b[39;00mcsv_file_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m process_json_files(tar_file_path)\n",
      "File \u001b[0;32m/mnt/c/Users/corey/Desktop/Education/University of Denver/Classes/COMP4447_Data Science Tools 1/Final Project/4447-Project/src/extract_data.py:12\u001b[0m, in \u001b[0;36mprocess_json_files\u001b[0;34m(tar_file_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_json_files\u001b[39m(tar_file_path):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m tarfile\u001b[39m.\u001b[39mopen(tar_file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m tar:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mfor\u001b[39;00m member \u001b[39min\u001b[39;00m tar\u001b[39m.\u001b[39;49mgetmembers():\n\u001b[1;32m     13\u001b[0m             \u001b[39mif\u001b[39;00m member\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     14\u001b[0m                 \n\u001b[1;32m     15\u001b[0m                 \u001b[39m# Extract data from JSON file\u001b[39;00m\n\u001b[1;32m     16\u001b[0m                 file_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(member\u001b[39m.\u001b[39mname)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:1822\u001b[0m, in \u001b[0;36mTarFile.getmembers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1820\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check()\n\u001b[1;32m   1821\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loaded:    \u001b[39m# if we want to obtain a list of\u001b[39;00m\n\u001b[0;32m-> 1822\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()        \u001b[39m# all members, we first have to\u001b[39;00m\n\u001b[1;32m   1823\u001b[0m                         \u001b[39m# scan the whole archive.\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmembers\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:2420\u001b[0m, in \u001b[0;36mTarFile._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2416\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read through the entire archive file and look for readable\u001b[39;00m\n\u001b[1;32m   2417\u001b[0m \u001b[39m   members.\u001b[39;00m\n\u001b[1;32m   2418\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 2420\u001b[0m     tarinfo \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext()\n\u001b[1;32m   2421\u001b[0m     \u001b[39mif\u001b[39;00m tarinfo \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2422\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:2342\u001b[0m, in \u001b[0;36mTarFile.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[39m# Advance the file pointer.\u001b[39;00m\n\u001b[1;32m   2341\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfileobj\u001b[39m.\u001b[39mtell():\n\u001b[0;32m-> 2342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfileobj\u001b[39m.\u001b[39;49mseek(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moffset \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m   2343\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfileobj\u001b[39m.\u001b[39mread(\u001b[39m1\u001b[39m):\n\u001b[1;32m   2344\u001b[0m         \u001b[39mraise\u001b[39;00m ReadError(\u001b[39m\"\u001b[39m\u001b[39munexpected end of data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:393\u001b[0m, in \u001b[0;36mGzipFile.seek\u001b[0;34m(self, offset, whence)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m READ:\n\u001b[1;32m    392\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_not_closed()\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer\u001b[39m.\u001b[39;49mseek(offset, whence)\n\u001b[1;32m    395\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset\n",
      "File \u001b[0;32m/usr/lib/python3.10/_compression.py:153\u001b[0m, in \u001b[0;36mDecompressReader.seek\u001b[0;34m(self, offset, whence)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39m# Read and discard data until we reach the desired position.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39mwhile\u001b[39;00m offset \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mmin\u001b[39;49m(io\u001b[39m.\u001b[39;49mDEFAULT_BUFFER_SIZE, offset))\n\u001b[1;32m    154\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    155\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:496\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    494\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread(io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 496\u001b[0m uncompress \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(buf, size)\n\u001b[1;32m    497\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    498\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mprepend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from extract_data import process_json_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
